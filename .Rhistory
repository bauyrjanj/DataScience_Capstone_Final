plot(m)
plot(m)
m<-gvisMotionChart(Fruits, "Fruits", "Year", options = list(width=600, height=400))
library(googleVis)
data("Fruits")
m<-gvisMotionChart(Fruits, "Fruits", "Year", options = list(width=600, height=400))
m<-gvisMotionChart(Fruits, "Fruit", "Year", options = list(width=600, height=400))
m<-gvisMotionChart(Fruits, "Fruit", "Year", options = list(width=600, height=400))
plot(m)
install.packages("leaflet")
library(leaflet)
my_map<-leaflet() %>% addTiles()
my_map
jasiko<-leaflet() %>% addTiles()
jasiko
library(leaflet)
my_map<-leaflet() %>%
addTiles()
my_map
library(leaflet)
library(dplyr)
my_map<-leaflet() %>%
addTiles() %>%
addMarkers(lat=39.2980803, lng=-76.5898801, popup="Shit")
my_map
library(leaflet)
library(dplyr)
my_map<-leaflet() %>%
addTiles() %>%
addMarkers(lat=48.398325, lng=89.662592, popup="Home")
my_map
library(leaflet)
library(dplyr)
my_map<-leaflet() %>%
addTiles() %>%
addMarkers(lat=48.970676, lng=89.967838, popup="Home")
my_map
df<-data.frame(lat=c(48.970676, 47.92, 32.092908), lng=c(89.967838, 106.92, 34.807216))
df<-leaflet() %>%
addTiles() %>%
addMarkers()
df<-data.frame(lat=c(48.970676, 47.92, 32.092908), lng=c(89.967838, 106.92, 34.807216))
df<-leaflet() %>%
addTiles() %>%
addMarkers()
df<-leaflet()
df<-leaflet() %>%
addTiles()
df<-leaflet() %>%
addTiles() %>%
addMarkers()
df<-data.frame(lat=c(47.921230, 49.537685, 44.891666), lng=c(106.918556, 105.960388, 110.136665))
df
df<-leaflet()
df<-leaflet() %>%
addTiles() %>%
addMarkers()
df %>% leaflet() %>%
addTiles() %>%
addMarkers()
df %>%
leaflet() %>%
addTiles() %>%
addMarkers()
df<-data.frame(lat=c(47.921230, 49.537685, 44.891666), lng=c(106.918556, 105.960388, 110.136665))
df %>% leaflet() %>% addTiles() %>% addMarkers()
icon<-makeIcon(iconURL="https://www.google.mn/search?q=bayan-ulgii+icon&espv=2&source=lnms&tbm=isch&sa=X&ved=0ahUKEwjPrZDih4bTAhXKmpQKHduNDpYQ_AUIBigB&biw=1260&bih=693#imgrc=N43eSIUvpuDLCM:",
iconWidth=31*215/230, iconHeight=31,
iconAnchorX=31*215/230/2, iconAnchorY=16)
icon<-makeIcon(iconURL="https://www.google.mn/search?q=bayan-ulgii+icon&espv=2&source=lnms&tbm=isch&sa=X&ved=0ahUKEwjPrZDih4bTAhXKmpQKHduNDpYQ_AUIBigB&biw=1260&bih=693#tbm=isch&q=bayan-ulgii+icon+png&*&imgrc=N43eSIUvpuDLCM:",
iconWidth=31*215/230, iconHeight=31,
iconAnchorX=31*215/230/2, iconAnchorY=16)
df<-data.frame(lat=c(47.921230, 49.537685, 44.891666), lng=c(106.918556, 105.960388, 110.136665))
df %>% leaflet() %>% addTiles() %>% addMarkers()
library(leaflet)
```{r, echo=FALSE}
library(leaflet)
library(dplyr)
df<-data.frame(lat=c(47.921230, 49.537685, 44.891666), lng=c(106.918556, 105.960388, 110.136665))
df %>% leaflet() %>% addTiles() %>% addMarkers()
library(leaflet)
df<-data.frame(lat=c(47.921230, 49.537685, 44.891666), lng=c(106.918556, 105.960388, 110.136665))
df %>% leaflet() %>% addTiles() %>% addMarkers()
library(leaflet)
df<-data.frame(lat=c(47.921230, 49.537685, 44.891666), lng=c(106.918556, 105.960388, 110.136665))
df %>% leaflet() %>% addTiles() %>% addMarkers()
version
version(knittr)
version(knitr)
version
help(package="knitr")
sessionInfo()
library(leaflet)
df<-data.frame(lat=c(47.921230, 49.537685, 44.891666), lng=c(106.918556, 105.960388, 110.136665))
df %>% leaflet() %>% addTiles() %>% addMarkers()
version
install.packages("evaluate")
install.packages("evaluate")
library(evaluate)
installed.packages()
library(pryr)
ftype(show)
ftype(mean)
ftype(predict)
ftype
ftype(predict)
ftype(lm)
ftype(colSums)
ftype(dgamma)
ftype(show)
ftype(mean)
showMethods("show")
library(pryr)
ftype
ftype(mean)
?caret
library(plotly)
data("mtcars")
head(mtcars)
head(mtcars)
plot_ly(mtcars, x=wt, y=mpg, mode="markers")
plot_ly(mtcars, x=wt, y=mpg, mode="markers")
plot_ly(x=mtcars$wt, y=mtcars$mpg, mode="markers")
plot_ly(x=mtcars$wt, y=mtcars$mpg, mode="markers", type = "scatter")
str(mtcars)
plot_ly(x=mtcars$wt, y=mtcars$mpg, mode="markers", type = "scatter", color=as.factor(mtcars$cyl))
plot_ly(x=mtcars$wt, y=mtcars$mpg, mode="markers", type = "scatter", color=as.factor(mtcars$cyl), size = mtcars$hp)
plot_ly(x=mtcars$wt, y=mtcars$mpg, z=mtcars$disp, mode="markers", type = "scatter3D", color=as.factor(mtcars$cyl), size = mtcars$hp)
plot_ly(x=mtcars$wt, y=mtcars$mpg, z=mtcars$disp, mode="markers", type = "scatter3d", color=as.factor(mtcars$cyl), size = mtcars$hp)
install.packages('rsconnect')
rsconnect::setAccountInfo(name='bauyrjanj',
token='DC1628980B58C17EC47B45DE44B8F2BF',
secret='<SECRET>')
rsconnect::setAccountInfo(name='bauyrjanj',
token='DC1628980B58C17EC47B45DE44B8F2BF',
secret='<SECRET>')
rsconnect::setAccountInfo(name='bauyrjanj',
token='DC1628980B58C17EC47B45DE44B8F2BF',
secret='<SECRET>')
library(rsconnect)
rsconnect::setAccountInfo(name='bauyrjanj',
token='DC1628980B58C17EC47B45DE44B8F2BF',
secret='<SECRET>')
rsconnect::setAccountInfo(name='bauyrjanj',
token='DC1628980B58C17EC47B45DE44B8F2BF',
secret='<SECRET>')
Sys.setlocale(locale="en_US.UTF-8")
rsconnect::setAccountInfo(name='bauyrjanj',
+ 			  token='DC1628980B58C17EC47B45DE44B8F2BF',
+ 			  secret='<SECRET>')
rsconnect::setAccountInfo(name='bauyrjanj',
token='DC1628980B58C17EC47B45DE44B8F2BF',
secret='<SECRET>')
rsconnect::setAccountInfo(name='bauyrjanj',
token='DC1628980B58C17EC47B45DE44B8F2BF',
secret='<SECRET>')
shiny::runApp('Documents/Coursera/Data Products/Assignment4')
runApp('Documents/Coursera/Data Products/Assignment4')
runApp('Documents/Coursera/Data Products/Assignment4')
runApp('Documents/Coursera/Data Products/Assignment4')
runApp('Documents/Coursera/Data Products/Assignment4')
runApp('Documents/Coursera/Data Products/Assignment4')
library(tm)
install.packages("nlp")
install.packages("NLP", dependencies = TRUE)
install.packages("NLP", dependencies = TRUE)
library(NLP)
library(tm)
library(RWekajars)
library(tokenizers)
words_twitter<-tokenize_words(twitter_lines, lowercase = TRUE)
twitter<-file("./final/en_US/en_US.twitter.txt","r")
twitter_lines<-readLines(twitter)
close(twitter)
words_twitter<-tokenize_words(twitter_lines, lowercase = TRUE)
test<-Corpus(VectorSource(twitter_lines))
testDF<-TermDocumentMatrix(test, control = words_twitter)
inspect(testDF)
library(NLP)
library(tm)
library(RColorBrewer)
library(wordcloud)
library(dplyr)
blogs<-file("./final/en_US/en_US.blogs.txt","r")
news<-file("./final/en_US/en_US.news.txt","r")
twitter<-file("./final/en_US/en_US.twitter.txt","r")
twitter_lines<-readLines(twitter)
close(twitter)
class(twitter_lines)
head(twitter_lines)
source_twitter<-VectorSource(twitter_lines)
class(source_twitter)
head(source_twitter)
length(source_twitter)
length(twitter_lines)
corpus_twitter<-Corpus(source_twitter)
head(corpus_twitter)
corpus_twitter<-tm_map(corpus_twitter, content_transformer(tolower))
corpus_twitter<-tm_map(corpus_twitter, removePunctuation)
corpus_twitter<-tm_map(corpus_twitter, stripWhitespace)
corpus_twitter<-tm_map(corpus_twitter, removeWords, stopwords("english"))
testDF<-TermDocumentMatrix(test)
inspect(testDF)
findFreqTerms(testDF, 10)
findFreqTerms(testDF, 1000)
length(testDF)
testDF
length(corpus_twitter)
findFreqTerms(testDF, 100000)
findFreqTerms(testDF, 10000)
summary(freq_words)
freq_words<-findFreqTerms(testDF, 10000)
summary(freq_words)
freq_words<-findFreqTerms(testDF, 20000)
freq_words
freq_words<-findFreqTerms(testDF)
freq_words
freq_words<-findFreqTerms(testDF, 20000)
freq_words<-findFreqTerms(testDF, 20000)
freq_words
findAssocs(testDF, "happy", 0.8)
library(rJava)
library(RWeka)
install.packages("rJava",type = "source")
install.packages("rJava", type = "source")
install.packages("RWeka")
library(RWeka)
library(rJava)
install.packages("RWeka")
library(RWeka)
library(RWekajars)
library(RWeka)
install.packages("openNLP")
suppressMessages(library(NLP))
suppressMessages(library(tm))
suppressMessages(library(RColorBrewer))
suppressMessages(library(wordcloud))
suppressMessages(library(dplyr))
suppressMessages(library(stringi))
suppressMessages(library(ggplot2))
suppressMessages(library(ngram))
suppressMessages(library(quanteda))
suppressMessages(library(gridExtra))
install.packages("irlba")
library(caret)
library(lattice)
library(e1071)
library(caret)
library(quanteda)
suppressMessages(library(NLP))
suppressMessages(library(tm))
suppressMessages(library(RColorBrewer))
suppressMessages(library(wordcloud))
suppressMessages(library(dplyr))
suppressMessages(library(stringi))
suppressMessages(library(RWeka))
suppressMessages(library(ggplot2))
suppressMessages(library(ngram))
suppressMessages(library(quanteda))
suppressMessages(library(gridExtra))
file1 <- "./final/en_US/en_US.blogs.txt"
file2 <- "./final/en_US/en_US.news.txt"
file3 <- "./final/en_US/en_US.twitter.txt"
# Read blogs
connect <- file(file1, open="rb")
blogs <- readLines(connect, encoding="UTF-8"); close(connect)
# Read news
connect <- file(file2, open="rb")
news <- readLines(connect, encoding="UTF-8"); close(connect)
# Read twitter
connect <- file(file3, open="rb")
twitter <- readLines(connect, encoding="UTF-8"); close(connect)
rm(connect)
l<-list(blogs, news, twitter)
summaryData <- sapply(l,function(x) summary(stri_count_words(x))[c('Min.','Mean','Max.')])
rownames(summaryData) <- c('Min','Mean','Max')
k<-sapply(l,stri_stats_general)[c('Lines','Chars'),]
w<-sapply(list(blogs,news,twitter),stri_stats_latex)['Words',]
stats <- data.frame(FileName=c("en_US.blogs","en_US.news","en_US.twitter"), t(rbind(k, Words=w, summaryData)))
head(stats)
blogs.size <- file.info(file1)$size / 1024 ^ 2
news.size <- file.info(file2)$size / 1024 ^ 2
twitter.size <- file.info(file3)$size / 1024 ^ 2
df<-data.frame(Doc = c("blogs", "news", "twitter"), Size.MB = c(blogs.size, news.size, twitter.size), Num.Lines = c(length(blogs), length(news), length(twitter)), Num.Words=c(sum(nchar(blogs)), sum(nchar(news)), sum(nchar(twitter))))
df
set.seed(123)
# Sampling Blogs
idxBlogs<-sample(1:length(blogs), 0.001*length(blogs), replace=FALSE)
sampleBlogs <- blogs[idxBlogs]
idxNews<-sample(1:length(news), 0.001*length(news), replace=FALSE)
sampleNews <- news[idxNews]
idxTwitter<-sample(1:length(twitter), 0.001*length(twitter), replace=FALSE)
sampleTwitter <- twitter[idxTwitter]
sampleBlogs <- iconv(sampleBlogs, "UTF-8", "ASCII", sub="")
sampleNews <- iconv(sampleNews, "UTF-8", "ASCII", sub="")
sampleTwitter <- iconv(sampleTwitter, "UTF-8", "ASCII", sub="")
data.sample <- c(sampleBlogs,sampleNews,sampleTwitter)
length(data.sample)
head(data.sample)
build_corpus <- function (x = data.sample) {
sample_c <- VCorpus(VectorSource(x)) # Create corpus dataset
sample_c <- tm_map(sample_c, content_transformer(tolower)) # all lowercase
sample_c <- tm_map(sample_c, removePunctuation) # Eleminate punctuation
sample_c <- tm_map(sample_c, removeNumbers) # Eliminate numbers
sample_c <- tm_map(sample_c, stripWhitespace) # Strip Whitespace
}
corpusData <- build_corpus(data.sample)
corpusData
etTermTable <- function(corpusData, ngrams = 1, lowfreq = 50) {
#create term-document matrix tokenized on n-grams
tokenizer <- function(x) { NGramTokenizer(x, Weka_control(min = ngrams, max = ngrams)) }
tdm <- TermDocumentMatrix(corpusData, control = list(tokenize = tokenizer))
#find the top term grams with a minimum of occurrence in the corpus
top_terms <- findFreqTerms(tdm,lowfreq)
top_terms_freq <- rowSums(as.matrix(tdm[top_terms,]))
top_terms_freq <- data.frame(word = names(top_terms_freq), frequency = top_terms_freq)
top_terms_freq <- arrange(top_terms_freq, desc(frequency))
}
tt.Data <- list(3)
for (i in 1:3) {
tt.Data[[i]] <- getTermTable(corpusData, ngrams = i, lowfreq = 10)
}
install.packages("rJava",type = "source")
library(rJava)
install.packages("rJava",type = "source")
library(rJava)
install.packages("rJava",type = "source")
library(rJava)
R.Version()
options(java.home="/System/Library/Frameworks/JavaVM.framework/Home")
library(rJava)
options(java.home="/Library/Internet Plug-Ins/JavaAppletPlugin.plugin/Contents/Home/bin/javae")
library(rJava)
Sys.setenv(JAVA_HOME='/Library/Internet Plug-Ins/JavaAppletPlugin.plugin/Contents/Home/bin/javae')
library(rJava)
JAVA_HOME='/Library/Internet Plug-Ins/JavaAppletPlugin.plugin/Contents/Home/bin/javae'
JAVA_HOME='/Library/Internet Plug-Ins/JavaAppletPlugin.plugin/Contents/Home/bin/java'
Sys.setenv(JAVA_HOME)
Sys.setenv(JAVA_HOME=JAVA_HOME)
library(rJava)
install.packages("rJava",type = "source")
library(rJava)
Sys.getenv("JAVA_HOME")
install.packages("rJava")
library(rJava)
R.version
install.packages("rJava")
library(rJava)
library(RWeka)
install.packages("RWeka")
library(RWeka)
suppressMessages(library(caret))
suppressMessages(library(kernlab))
suppressMessages(library(ggplot2))
suppressMessages(library(dplyr))
suppressMessages(library(AppliedPredictiveModeling))
suppressMessages(library(pgmm))
suppressMessages(library(rpart))
suppressMessages(library(gbm))
suppressMessages(library(lubridate))
suppressMessages(library(ElemStatLearn))
suppressMessages(library(forecast))
suppressMessages(library(e1071))
data("spam")
head(spam)
dim(spam)
inTrain<-createDataPartition(y=spam$type, p=0.75, list = FALSE)
names(spam)
head(spam$spam)
inTrain<-createDataPartition(y=spam$spam, p=0.75, list = FALSE)
training<-spam[inTrain,]
test<-spam[-inTrain,]
table(spam$spam)
data("mtcars")
library(mtcars)
head(mtcars)
library(RWeka)
library(RWekajars)
library(RWeka)
suppressMessages(library(NLP))
suppressMessages(library(tm))
suppressMessages(library(RColorBrewer))
suppressMessages(library(wordcloud))
suppressMessages(library(dplyr))
suppressMessages(library(stringi))
suppressMessages(library(ggplot2))
suppressMessages(library(ngram))
suppressMessages(library(quanteda))
suppressMessages(library(gridExtra))
suppressMessages(library(stringr))
suppressMessages(library(data.table))
file1 <- "./final/en_US/en_US.blogs.txt"
file2 <- "./final/en_US/en_US.news.txt"
file3 <- "./final/en_US/en_US.twitter.txt"
# Read blogs
connect <- file(file1, open="rb")
blogs <- readLines(connect, encoding="UTF-8"); close(connect)
# Read news
connect <- file(file2, open="rb")
news <- readLines(connect, encoding="UTF-8"); close(connect)
# Read twitter
connect <- file(file3, open="rb")
twitter <- readLines(connect, encoding="UTF-8"); close(connect)
rm(connect)
set.seed(123)
# Sampling Blogs
idxBlogs<-sample(1:length(blogs), 0.001*length(blogs), replace=FALSE)
sampleBlogs <- blogs[idxBlogs]
# Sampling News
idxNews<-sample(1:length(news), 0.001*length(news), replace=FALSE)
sampleNews <- news[idxNews]
# Sampling Twitter
idxTwitter<-sample(1:length(twitter), 0.001*length(twitter), replace=FALSE)
sampleTwitter <- twitter[idxTwitter]
# Cleaning
sampleBlogs <- iconv(sampleBlogs, "UTF-8", "ASCII", sub="")
sampleNews <- iconv(sampleNews, "UTF-8", "ASCII", sub="")
sampleTwitter <- iconv(sampleTwitter, "UTF-8", "ASCII", sub="")
data.sample <- c(sampleBlogs,sampleNews,sampleTwitter)
length(data.sample)
build_corpus <- function (x = data.sample) {
sample_c <- VCorpus(VectorSource(x)) # Create corpus dataset
sample_c <- tm_map(sample_c, content_transformer(tolower)) # all lowercase
sample_c <- tm_map(sample_c, removePunctuation) # Eleminate punctuation
sample_c <- tm_map(sample_c, removeNumbers) # Eliminate numbers
sample_c <- tm_map(sample_c, stripWhitespace) # Strip Whitespace
}
corpusData <- build_corpus(data.sample)
ng1<-function(x) unlist(lapply(ngrams(words(x), 1), paste, collapse = " "), use.names = TRUE)
ng2<-function(x) unlist(lapply(ngrams(words(x), 2), paste, collapse = " "), use.names = TRUE)
ng3<-function(x) unlist(lapply(ngrams(words(x), 3), paste, collapse = " "), use.names = TRUE)
Tdm1g<-TermDocumentMatrix(corpusData, control = list(tokenize = ng1))
Tdm1gns<-removeSparseTerms(Tdm1g, 0.999)
# Create a frequency matrix
freq1g<-rowSums(as.matrix(Tdm1gns))
# Sort the frequency matrix in descending order
freq1g<-sort(freq1g, decreasing = TRUE)
save(freq1g, file = "freq1.f1.RData")
dff1g<-data.frame("Term"=names(head(freq1g)), "Frequency"=head(freq1g))
rownames(dff1g)<-c(1:nrow(dff1g))
dff1g<-data.table(dff1g)
save(dff1g, file = "freq1.f1.RData")
Tdm2g<-TermDocumentMatrix(corpusData, control = list(tokenize = ng2))
Tdm2gns<-removeSparseTerms(Tdm2g, 0.999)
freq2g<-rowSums(as.matrix(Tdm2gns))
freq2g<-sort(freq2g, decreasing = TRUE)
dff2g<-data.frame("Term"=names(head(freq2g)), "Frequency"=head(freq2g))
rownames(dff2g)<-c(1:nrow(dff2g))
dff2g<-data.table(dff2g)
dff2g$Term<-as.character(dff2g$Term)
dff2g$Words<-str_replace_all(dff2g$Term, "[[:punct:]]", " ")
library(qdap)
dff2g$w1<-word(dff2g$Words, start=1, sep = " ")
dff2g$w2<-word(dff2g$Words, start=2, sep = " ")
save(dff2g, file = "freq2.f2.RData")
Tdm3g<-TermDocumentMatrix(corpusData, control = list(tokenize = ng3))
shiny::runApp('Documents/Coursera/Capstone')
runApp('Documents/Coursera/Capstone')
runApp('Documents/Coursera/Capstone')
runApp('Documents/Coursera/Capstone')
runApp('Documents/Coursera/Capstone')
runApp('Documents/Coursera/Capstone')
runApp('Documents/Coursera/Capstone')
runApp('Documents/Coursera/Capstone')
runApp('Documents/Coursera/Capstone')
runApp('Documents/Coursera/Capstone')
runApp('Documents/Coursera/Capstone')
traceback()
runApp('Documents/Coursera/Capstone')
runApp('Documents/Coursera/Capstone')
runApp('Documents/Coursera/Capstone')
runApp('Documents/Coursera/Capstone')
runApp('Documents/Coursera/Capstone')
runApp('Documents/Coursera/Capstone')
runApp('Documents/Coursera/Capstone')
runApp('Documents/Coursera/Capstone')
library(shiny)
library(data.table)
library(NLP)
library(tm)
library(qdap)
obs<-"I want"
as.numeric(wc(obs))
f<-as.numeric(wc(obs))
class(f)
runApp('Documents/Coursera/Capstone')
runApp('Documents/Coursera/Capstone')
runApp('Documents/Coursera/Capstone')
runApp('Documents/Coursera/Capstone')
runApp('Documents/Coursera/Capstone')
shiny::runApp('Documents/Coursera/Capstone')
shiny::runApp('Documents/Coursera/Capstone')
runApp('Documents/Coursera/Capstone')
setwd("~/Documents/Coursera/Capstone")
shiny::runApp()
runApp()
runApp()
runApp()
runApp()
load("nfreq.v5.RData")
lengths(ngramsList)
length(ngramsList)
showReactLog()
setwd("~/Documents/Coursera/Capstone")
setwd("~/Desktop/Final")
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
